{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOADR9GNQxRdhOj1Bn/Cjtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwelland/ENGPYHS_3NM4/blob/main/Curve_fit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goals\n",
        "* Understand how to obtain the 'best fit' of data for the linear case\n",
        "* Be able to use calculate and use the pseudoinverse\n",
        "* Watch out and exploit weighting"
      ],
      "metadata": {
        "id": "SLZA16TLAoEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Least squares regression"
      ],
      "metadata": {
        "id": "vrwTllzfM2k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, we will have some datapoint pairs $x_i, y_i$. We will have some function $y$ with parameters $a$ that will be evaluated at a point $x$: $y(a, x)$.\n",
        "\n",
        "Our goal is to find the set of parameters $a$ that gives us the *best fit* of the data. Commonly, this implies minimizing the squared error between the prediction and the data,\n",
        "\n",
        "$$y(a,x_i)-y_i = r_i$$.\n",
        "\n",
        "where $r$ is the residual vector. The *least squares fit* is formulated as finding $a$ so as to minimize $\\| r\\|$.\n",
        "\n",
        "In general, this is an optimization problem (much more complicated than you would expect!) since $y(a,x)$ can be complex."
      ],
      "metadata": {
        "id": "LOopE2SkM5O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear least squares regression"
      ],
      "metadata": {
        "id": "gHpilsc7IsPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the *simpler* case first; in particular where $y$ is the sum of basis functions weighted by $a$ (e.g.: polynomial interpolation, radial basis functions... can you think of another one?).\n",
        "\n",
        "In this case, $y=Ax$, and the data is inserted into $b$.\n",
        ">CAUTION! We have swapped notation to follow suit with standard practice! The matrix A is the function of the 'position', $x$, and the parameters are in the vector!"
      ],
      "metadata": {
        "id": "DAv_l0gzQf5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a matrix system,\n",
        "$$ A x = b$$\n",
        "where $A$ is an $m\\times n$ matrix, $x$ is $n$, and $b$ is $$. We cannot solve this for an exact $x$ with our normal techniques since $A$ is rectangular, not square.\n",
        "\n",
        "Recalling the residual is $Ax-b$, let's broaden our concept to a 'solution' to say we want to minimize the (norm of the) residual.\n",
        "\n",
        "\n",
        "$$Min_x \\ of \\ \\| Ax-b\\|=\\frac{1}{2} [Ax-b]^T [Ax-b]$$\n",
        "\n",
        "Setting $\\frac{d}{dx} = 0$, we get:\n",
        "\n",
        "\\begin{align}\n",
        "2 A^T [Ax-b] &= 0 \\\\\n",
        "A^T A x &= A^T b \\\\\n",
        " x &= [A^T A]^{-1} A^T b \\\\\n",
        "x &= A^† b\n",
        "\\end{align}\n",
        "\n",
        "where $A^†=[A^T A]^{-1} A^T$ is called the *(Moore-Penrose) pseudoinverse* of $A$. The pseudoinverse is defined for any rectangular matrix. Note $A^T A$ is necessarily square, and is generally invertible.\n",
        "\n",
        "* The pseudoinverse is defined for any rectangular matrix\n",
        "* When used to solve $Ax=b$ it results in the *best fit* (in the least squares sense)\n",
        "* Since the ultimate *minimum* is $0$, the pseudoinverse is the true inverse for an exactly solvable system.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oQf0n7zEIvF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conditioning of a rectangular matrix"
      ],
      "metadata": {
        "id": "UyF7RZuDOEoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The determinant of a rectangual matrix is undefined, but we can resort to the definition of the condition number:\n",
        "$$cond(A) = \\|A\\| \\|A^\\dagger\\|$$"
      ],
      "metadata": {
        "id": "msndImUtOI9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terminology\n",
        "* A **consistent** system of equations has a solution that satisfies *all* the equations.\n",
        "* An **inconsistent** system has no solution that satisfies all equations simultaneously.\n",
        "\n",
        "* * **Overdetermined** systems have more equations that unknowns which is typical of curve fitting. These systems are inconsistent in that there is *no simultaneous solution*, but a solution does exists that *simultaneously minimizes the error*.  \n",
        "* * **Underdetermined** systems have fewer equations than unknowns and are also inconsistent but with an *infinite* number of solutions. E.g.: Parallel lines / 2 equations with 3 variables."
      ],
      "metadata": {
        "id": "jjROs66WFyIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Example: An overdetermined, consistent linear system (our headscratcher!)"
      ],
      "metadata": {
        "id": "zwglkl28FBi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$20 c + 50 t = 700$\n",
        "\n",
        "$  c+t = 20$\n",
        "\n",
        "$ 50 c + 20 t = 700$\n",
        "\n",
        "which gives the linear system:\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        " 20 & 50  \\\\\n",
        " 1 & 1 \\\\\n",
        " 50 & 20\n",
        " \\end{bmatrix}\n",
        " \\begin{bmatrix}\n",
        " c \\\\\n",
        " t\n",
        " \\end{bmatrix} =\n",
        " \\begin{bmatrix}\n",
        " 700 \\\\\n",
        " 20 \\\\\n",
        " 700\n",
        " \\end{bmatrix}\n",
        " $$"
      ],
      "metadata": {
        "id": "l4B_aN5TlH12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot it!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the x values\n",
        "x = np.linspace(0, 20, 100)\n",
        "\n",
        "# Calculate the y values for the first equation (20c + 50t = 700)\n",
        "y1 = (700 - 20 * x) / 50\n",
        "\n",
        "# Calculate the y values for the second equation (c + t = 20)\n",
        "y2 = 20 - x\n",
        "\n",
        "# Calculate the y values for the third equation (50c + 20t = 700)\n",
        "y3 = (700 - 50 * x) / 20\n",
        "\n",
        "# Plot the lines\n",
        "plt.plot(x, y1, label='20c + 50t = 700')\n",
        "plt.plot(x, y2, label='c + t = 20')\n",
        "plt.plot(x, y3, label='50c + 20t = 700')\n",
        "\n",
        "# Add a grid\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "G_vjdVBklmep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The arrays are:\n",
        "# ~~ Question - what is the linear system and how do we solve it?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ~~ Answer\n",
        "# A = np.array([[20, 50], [1, 1], [50, 20]])\n",
        "# b = np.array([700, 20, 700])\n",
        "\n",
        "# #x = np.linalg.solve(A, b)\n",
        "\n",
        "# M = np.linalg.inv(A.T @ A)@A.T\n",
        "# print(M)\n",
        "# print(np.linalg.pinv(A))\n",
        "# print(M-np.linalg.pinv(A))\n",
        "# print(M@b)\n"
      ],
      "metadata": {
        "id": "qfAJuzxsl6SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: An overdetermined, inconsistent linear system\n"
      ],
      "metadata": {
        "id": "BcTtXqOR5eFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$20 c + 50 t = 700$\n",
        "\n",
        "$  c+t = 20$\n",
        "\n",
        "$ 60 c + 20 t = 700$"
      ],
      "metadata": {
        "id": "_2AETNrc5nxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the x values\n",
        "x = np.linspace(7, 10, 100)\n",
        "\n",
        "# Calculate the y values for the first equation (20c + 50t = 700)\n",
        "y1 = (700 - 20 * x) / 50\n",
        "\n",
        "# Calculate the y values for the second equation (c + t = 20)\n",
        "y2 = 20 - x\n",
        "\n",
        "# Calculate the y values for the third equation (60c + 20t = 700)\n",
        "y3 = (700 - 60 * x) / 20\n",
        "\n",
        "# Plot the lines\n",
        "plt.plot(x, y1, label='20c + 50t = 700')\n",
        "plt.plot(x, y2, label='c + t = 20')\n",
        "plt.plot(x, y3, label='60c + 20t = 700')\n",
        "\n",
        "plt.xlabel('c')\n",
        "plt.ylabel('t')\n",
        "plt.title('Least Squares Solution for Inconsistent System')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ytojeY854gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where do you think he solution is going to be?"
      ],
      "metadata": {
        "id": "4uoVlkaPUdvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[20, 50], [1, 1], [60, 20]])\n",
        "b = np.array([700, 20, 700])\n",
        "\n",
        "#np.linalg.(A, b)\n",
        "x_lsq = np.linalg.pinv(A)@b\n",
        "print(x_lsq)\n",
        "\n",
        "# Plot the lines\n",
        "plt.plot(x, y1, label='20c + 50t = 700')\n",
        "plt.plot(x, y2, label='c + t = 20')\n",
        "plt.plot(x, y3, label='60c + 20t = 700')\n",
        "\n",
        "\n",
        "plt.plot(x_lsq[0], x_lsq[1], 'ro', label='Least Squares Solution')\n",
        "plt.xlabel('c')\n",
        "plt.ylabel('t')\n",
        "plt.title('Least Squares Solution for Inconsistent System')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZpGwQQelUXW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Was this what you were expecting?"
      ],
      "metadata": {
        "id": "UarH8ahmUhHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Weights"
      ],
      "metadata": {
        "id": "SFEqmxkTUqGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with most approximate methods, the moment we start to move away from an *exact solution* subtle effects start to show up!\n",
        "\n",
        "Notice we are minimizing the *residuals* but there is a subtle problem with the problem definition above:\n",
        "\n",
        "$$\\begin{align}\n",
        "20 c + 50 t &= 700  \\\\\n",
        "c+t  &= 20 \\\\\n",
        "60 c + 20 t &= 700\n",
        "\\end{align} $$\n",
        "\n",
        "The coefficients of the second equation is about an order of magnitude lower than the others. Of course this system is equivilant to:\n",
        "\n",
        "$$\\begin{align}\n",
        "20 c + 50 t &= 700  \\\\\n",
        "10c+10t  &= 200 \\\\\n",
        "60 c + 20 t &= 700\n",
        "\\end{align} $$\n",
        "\n",
        "or even\n",
        "\n",
        "$$\\begin{align}\n",
        "0.2 c + 0.5 t &= 7  \\\\\n",
        "c+t  &= 20 \\\\\n",
        "0.6 c + 0.2 t &= 7\n",
        "\\end{align} $$\n",
        "\n",
        "What does this remind you of?\n"
      ],
      "metadata": {
        "id": "LJYqmeARUtUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobi (diagonal) preconditioning!\n",
        "\n",
        "$$P^{-1} = \\begin{bmatrix} W_{1} & 0 & 0 \\\\ 0 & W_{2} & 0 \\\\ 0 & 0 & W_{3} \\end{bmatrix}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "UIqIXTwHXVuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we define $r_1, r_2, r_3$:\n",
        "\n",
        "$$\\begin{align}\n",
        "20 c + 50 t -700 &= r_1 \\\\\n",
        "c+t -20 &= r_2 \\\\\n",
        "60 c + 20 t -700 &= r_3\n",
        "\\end{align} $$\n",
        "\n",
        "We say the residuals are / can be *weighted*, i.e.: the least squares problem becomes,\n",
        "\n",
        "$$ Min_x \\ of \\ \\sum W_i^2 r_i $$\n",
        "\n",
        "Let's code it!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h0OATdb1WmjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[20, 50], [1, 1], [60, 20]])\n",
        "b = np.array([700, 20, 700])\n",
        "\n",
        "#~~ Question: What's the preconditioner? How do we apply it?\n",
        "\n",
        "\n",
        "#~~ Answer\n",
        "# Pi = np.diag([1,70,1])\n",
        "# print(Pi)\n",
        "# A = Pi@A\n",
        "# b = Pi@b\n",
        "###\n",
        "\n",
        "\n",
        "\n",
        "x_lsq = np.linalg.pinv(A)@b\n",
        "print(x_lsq)\n",
        "\n",
        "# Plot the lines\n",
        "plt.plot(x, y1, label='20c + 50t = 700')\n",
        "plt.plot(x, y2, label='c + t = 20')\n",
        "plt.plot(x, y3, label='60c + 20t = 700')\n",
        "\n",
        "\n",
        "plt.plot(x_lsq[0], x_lsq[1], 'ro', label='Least Squares Solution')\n",
        "plt.xlabel('c')\n",
        "plt.ylabel('t')\n",
        "plt.title('Least Squares Solution for Inconsistent System')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bTjuqIUdYSfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How would we want to use weights in the future? (Think data measurement and uncertainty!)"
      ],
      "metadata": {
        "id": "IBd1AxmibpJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polyfit"
      ],
      "metadata": {
        "id": "tYSG2ZDXjrwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's return to our polynomial fitting armed with our new tool, and use it to separate the order of the polynomial from the number of data points.\n",
        "\n",
        "An $n$th degree polynomial,\n",
        "\n",
        "$$ y(x) = a_n x^n + a_{n-1} x^{n-1} \\dots a_2 x^2 + a_1 x +a_0$$\n",
        "\n",
        "can be applied to $m$ data points,\n",
        "\n",
        "$y(x_i) = a_n x_i^n + a_{n-1} x_i^{n-1} \\dots a_2 x_i^2 + a_1 x_i +a_0 = y_i$\n",
        "\n",
        "to generate an $m \\times n$ matrix, multiplied by an $n$ vector of polynomial coefficients to equal an $m$ vector of data:\n",
        "\n",
        "$$\n",
        " \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\\n",
        "1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_m & x_m^2 & \\cdots & x_m^n\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_0 \\\\\n",
        "a_1 \\\\\n",
        "a_2 \\\\\n",
        "\\vdots \\\\\n",
        "a_n\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "y_3 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "G7X4TIrujvLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Determine the coefficients of a cubic polynomial"
      ],
      "metadata": {
        "id": "lxfLkxFzNQu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate 100 samples of the function 3x^4-2x^2+x-9 with +-100 noise. Plot the true curve with dashed lines and the data with small points in red.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Define the true function\n",
        "def true_function(x):\n",
        "  return 3 * x**4 - 2 * x**2 + x - 9\n",
        "\n",
        "# Calculate the true y values\n",
        "y_true = true_function(x)\n",
        "\n",
        "# Generate noisy data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "noise = np.random.normal(0, 100, 100)\n",
        "x_data = np.linspace(-5, 5, 100)\n",
        "y_data = true_function(x_data) + noise\n",
        "\n",
        "# Plot the true curve and the data\n",
        "plt.plot(x, y_true, '--', label='True Function')\n",
        "plt.plot(x_data, y_data, 'ro', markersize=3, label='Noisy Data')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('True Function and Noisy Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bGy87ajvJZ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate the vermonde matrix for a cubic polynomial, invert it using pinv and the y data to find the coefficients of the polynomial and then plot it along with the data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Generate the Vandermonde matrix for a cubic polynomial\n",
        "n = 3  # Degree of the polynomial\n",
        "X = np.vander(x_data, n + 1)\n",
        "\n",
        "# Calculate the coefficients using the pseudoinverse\n",
        "coefficients = np.linalg.pinv(X) @ y_data\n",
        "\n",
        "coeffs = np.polyfit(x_data, y_data, 3)\n",
        "print('Coefficients calculated manually', coefficients, '\\n')\n",
        "print('Coefficients calculated with polyfit', coeffs)\n",
        "\n",
        "# Generate y values for the fitted polynomial\n",
        "fitted_polynomial = np.poly1d(coeffs)\n",
        "y_fitted = fitted_polynomial(x)\n",
        "\n",
        "\n",
        "# Plot the fitted polynomial along with the data\n",
        "plt.plot(x, y_fitted, label='Fitted Polynomial')\n",
        "plt.plot(x_data, y_data, 'ro', markersize=3, label='Noisy Data')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Fitted Polynomial and Noisy Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wVU7FYVnKDac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: Find a series of 'best fit polynomials'"
      ],
      "metadata": {
        "id": "7VoAXXa1NfP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: parameterize polyfit for degrees 0 to 10, plotting each on the same plot along with the sample data. Output the norm of the residuals for each fit along with the coefficients to 2 decimal places\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Define the true function\n",
        "def true_function(x):\n",
        "  return 3 * x**4 - 2 * x**2 + x - 9\n",
        "\n",
        "# Generate noisy data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "noise = np.random.normal(0, 100, 100)\n",
        "x_data = np.linspace(-5, 5, 100)\n",
        "y_data = true_function(x_data) + noise\n",
        "\n",
        "# Plot the true curve and the data\n",
        "plt.plot(x_data, y_data, 'ro', markersize=3, label='Noisy Data')\n",
        "\n",
        "for degree in range(11):\n",
        "    coeffs = np.polyfit(x_data, y_data, degree)\n",
        "    fitted_polynomial = np.poly1d(coeffs)\n",
        "    y_fitted = fitted_polynomial(x)\n",
        "    plt.plot(x, y_fitted, label=f'Degree {degree}')\n",
        "    residuals = y_data - fitted_polynomial(x_data)\n",
        "    residual_norm = np.linalg.norm(residuals)\n",
        "    print(f\"Degree {degree}: Residual Norm = {residual_norm:.2f}, Coefficients = {[round(c, 2) for c in coeffs]}\")\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Fitted Polynomials of Varying Degrees')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aOc_RQdsNnJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting points: The curve actually is 4th order but because additional terms will always reduce the error, it is not trivial to tell which is the *best*, best fit!"
      ],
      "metadata": {
        "id": "cL56XYySOqEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 3: Consider the condition number of the Vermonde matrix for increasing n"
      ],
      "metadata": {
        "id": "fo6-6NvcP1pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the condition number for an increasing series of vermonde matricies on 10 data points\n",
        "\n",
        "import numpy as np\n",
        "# Generate x values\n",
        "x_data = np.linspace(-5, 5, 10)\n",
        "\n",
        "for degree in range(1, 11):\n",
        "    # Generate the Vandermonde matrix for a given degree\n",
        "    X = np.vander(x_data, degree + 1)\n",
        "\n",
        "    # Calculate the condition number\n",
        "    condition_number = np.linalg.cond(X)\n",
        "\n",
        "    print(f\"Degree {degree}: Condition Number = {condition_number:.2f}\")\n"
      ],
      "metadata": {
        "id": "luq6Nr-KP-1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that as the condition number strays from 1, numerical algorithms deteriorate. This is why low-order polynomials are more numerically robust to fit than high order!"
      ],
      "metadata": {
        "id": "8mku53BkQir8"
      }
    }
  ]
}